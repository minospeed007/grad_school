{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8364b3e4-95bd-408f-9ca6-738f77e6ffb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0edbee2-ecfb-4fa5-9e5d-973798de32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train= pd.read_csv(\"kdd_train.csv\")\n",
    "df_test= pd.read_csv(\"kdd_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0ea8b1-681b-460d-8eac-d765fd742418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90955935-206e-40d7-9888-ae398cf7f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train= df_train.drop(\"labels\", axis=1)\n",
    "y_train= df_train[\"labels\"]\n",
    "X_test= df_test.drop(\"labels\", axis=1)\n",
    "y_test= df_test[\"labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c011c8b-a694-4732-98d4-8448fc6e09da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score RF 0.9139017033356991\n",
      "svmem(total=4151689216, available=707395584, percent=83.0, used=3444293632, free=707395584)\n"
     ]
    }
   ],
   "source": [
    "categorical_cols= [\"protocol_type\", \"service\", \"flag\"]\n",
    "preprocessor= ColumnTransformer(transformers=[\n",
    "    (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_cols),\n",
    "    (\"num\", StandardScaler(), [col for col in X_train.columns if col not in categorical_cols])\n",
    "])\n",
    "model_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"classifier\", RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "y_pred= model_pipeline.predict(X_test)\n",
    "print(\"Accuracy score RF\", accuracy_score(y_test, y_pred))\n",
    "import psutil\n",
    "print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "962e05ae-550b-46dc-be55-76519f2f8370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79c49ace-a8ea-4658-b22e-b53950b95b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = model_pipeline.named_steps[\"classifier\"].feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41cffb58-0e80-4ca3-905c-bcd7b2b444ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_columns = model_pipeline.named_steps[\"preprocessor\"].transformers_[0][1].get_feature_names_out(input_features=[\"protocol_type\", \"service\", \"flag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d485c36-caa9-4adb-822e-268d1780b915",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col for col in X_train.columns if col not in [\"protocol_type\", \"service\", \"flag\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00b668ea-531a-4ab7-89bb-7741dbaed4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(one_hot_columns) + numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0699b296-3f0a-4e3d-87b3-ef6a75372d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame({\n",
    "    \"feature\": all_features,\n",
    "    \"importance\": importances\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "top_features = feature_importance_df[\"feature\"].iloc[:20].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e93047-1651-4b4b-abc7-e43a00946830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_features(df, top_feat_list):\n",
    "    df_encoded = pd.get_dummies(df, columns=categorical_cols)\n",
    "    return df_encoded[top_feat_list]\n",
    "X_train_reduced = reduce_features(X_train, top_features)\n",
    "X_test_reduced = reduce_features(X_test, top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b1be2f0-a61e-422d-a273-d2d6bf6b2d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaF Feature selection score: 0.7231192334989354\n",
      "svmem(total=4151689216, available=656621568, percent=84.2, used=3495067648, free=656621568)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "base_model= DecisionTreeClassifier(max_depth=1)\n",
    "model = AdaBoostClassifier( \n",
    "    estimator=base_model,\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "model.fit(X_train_reduced, y_train)\n",
    "y_pred= model.predict(X_test_reduced)\n",
    "print(\"AdaF Feature selection score:\", accuracy_score(y_test,y_pred))\n",
    "import psutil\n",
    "print(psutil.virtual_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517c1e6f-affb-4124-802d-a5b608241000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:776: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\USER\\miniconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier,StackingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "label_encoder= LabelEncoder()\n",
    "y_train_encoded= label_encoder.fit_transform(y_train)\n",
    "valid_mask = y_test.isin(label_encoder.classes_)\n",
    "X_test_filtered = X_test_reduced[valid_mask]\n",
    "y_test_filtered = y_test[valid_mask]\n",
    "\n",
    "# Encode filtered test labels\n",
    "y_test_encoded = label_encoder.transform(y_test_filtered)\n",
    "base_model= DecisionTreeClassifier(max_depth=2)\n",
    "base_learners=[\n",
    "    (\"RF\", RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    (\"ADA\", AdaBoostClassifier(estimator=base_model, random_state=42))\n",
    "]\n",
    "meta_learner=LogisticRegression()\n",
    "stack_model = StackingClassifier( \n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    passthrough=True,\n",
    ")\n",
    "stack_model.fit(X_train_reduced, y_train_encoded)\n",
    "y_pred= stack_model.predict(X_test_filtered)\n",
    "accuracy= accuracy_score(y_test_encoded, y_pred)\n",
    "print(f\"XGB Accuracy score: {accuracy:.3f}\")\n",
    "print(psutil.virtual_memory())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fbbf6d-db62-43b6-8a79-7a6da7093eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_selection import mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad332a3-c485-4e0e-bb87-669555321d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = mutual_info_classif(X_train_reduced, y_train_encoded)\n",
    "mi_series = pd.Series(mi, index=X_train_reduced.columns).sort_values(ascending=False)\n",
    "corr_matrix = X_train_reduced.corr().abs()\n",
    "G= nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a6fe0e-40b5-42cf-a3ac-d88c49eac3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "G.add_nodes_from(X_train_reduced.columns[:20])\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if corr_matrix.iloc[i, j] > 0.85:  # set your threshold\n",
    "            G.add_edge(corr_matrix.columns[i], corr_matrix.columns[j])\n",
    "components = list(nx.connected_components(G))\n",
    "clusters = [list(c) for c in components if len(c) > 1]\n",
    "independents = [list(c)[0] for c in components if len(c) == 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c114a38-5ae8-40e4-9abe-138ce496dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop= set()\n",
    "for component in nx.connected_components(G):\n",
    "    if len(component) > 1:\n",
    "        top_feature = max(component, key=lambda f: mi_series[f])\n",
    "        features_to_drop.update(set(component) - {top_feature})\n",
    "X_graph_filtered = X_train_reduced.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cd3b4-7b24-4752-854b-139dfeff2163",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_scores = pd.Series(\n",
    "    mutual_info_classif(X_train_reduced, y_train_encoded),\n",
    "    index=X_train_reduced.columns\n",
    ")\n",
    "# Most informative feature from each cluster\n",
    "best_from_clusters = []\n",
    "for group in clusters:\n",
    "    group_scores = mi_scores[group]\n",
    "    best_feature = group_scores.idxmax()\n",
    "    best_from_clusters.append(best_feature)\n",
    "# Combine independent features and selected bests from clusters\n",
    "final_features = list(set(independents + best_from_clusters))\n",
    "X_selected = X_train_reduced[final_features]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a211d86-8a6b-49ac-b7cc-9ed2cbd86c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_selected = X_train_reduced[final_features]\n",
    "X_test_selected = X_test_reduced[final_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f3c6ac-b097-4e26-aac0-aae4af30de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(\n",
    "    eval_metric='mlogloss',\n",
    "    random_state=42,\n",
    "    use_label_encoder=False  # for newer versions of xgboost\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5e7b26-3585-44c8-a150-de88c21d5ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_filtered = X_test_reduced[valid_mask]\n",
    "\n",
    "# Then select only the final features from your cleaned subset\n",
    "X_test_selected = X_test_filtered[final_features]\n",
    "\n",
    "# Now safely realign labels\n",
    "y_test_filtered = y_test[valid_mask]\n",
    "y_test_encoded = label_encoder.transform(y_test_filtered)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b686542-a78d-48d5-94ec-688d35af5b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          eval_set=[(X_test, y_test)],\n",
    "          verbose=False)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy with refined features: {accuracy:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5197146c-71e9-47a7-aac2-b193126157f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def benchmark_model(X_train, y_train, X_test, y_test, label=\"Model\"):\n",
    "    process = psutil.Process()\n",
    "    start_mem = process.memory_info().rss / (1024 ** 2)  # in MB\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = xgb.XGBClassifier(\n",
    "        eval_metric='mlogloss',\n",
    "        use_label_encoder=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    end_time = time.time()\n",
    "    end_mem = process.memory_info().rss / (1024 ** 2)\n",
    "\n",
    "    print(f\" {label} -- Accuracy: {acc:.4f}\")\n",
    "    print(f\" Runtime: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\" Memory Used: {end_mem - start_mem:.2f} MB\\n\")\n",
    "\n",
    "# Run for full feature set\n",
    "benchmark_model(X_train_reduced, y_train_encoded, X_test_reduced, y_test_encoded, label=\"Full Features\")\n",
    "\n",
    "# Run for pruned feature set\n",
    "benchmark_model(X_train_selected, y_train_encoded, X_test_selected, y_test_encoded, label=\"Pruned Features\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
